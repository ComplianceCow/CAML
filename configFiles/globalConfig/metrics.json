{
    "metrics": [
        {
            "controlID": "AIS-06",
            "metricID": "AIS-06-M1",
            "metricDescription": "This metric measures the percentage of running production code that can be directly traced back to automated security and quality tests that verify the compliance of each build.",
            "metricExpression": "Percentage of compliant code: 100 * A/B\nWhere:\nA = Total number of pieces of Production Code that have an Associated Verification Step\nB = Total number of pieces of Production Code",
            "metricRule": "\"Production Code\" is code deployed to the production runtime environment(s) within the scope of the Information Security Program defined in the CCMv4 GRC-05 control objective.\n\nAn \"Associated Verification Step\" is a capability in the deployment process that ties production code back to a build with traceable results for quality, security, and privacy tests.",
            "metricImplementationGuidelines": "There must be a Software Inventory of Deployed Production Code (DCS-06).  Production code must be quantified based on the organization's definition of deployed code running in production (e.g. microservices, builds, releases, packages, libraries, serverless functions, etc.). This should be the same number used to measure AIS-07.\n\nThe definition of \"deployed production code\" used for the software inventory should be aligned with application security scanning, testing, and/or reporting methods where possible to simplify measurement.\n\nThe likelihood of standardized deployments can decrease as the number of different deployment systems increases. If the Software Deployment Pipeline has multiple stages where change could be introduced, and end-to-end validation cannot be performed, then this metric may be more suitable for an organization:\n\n0%<=Percentage of steps in the Software Deployment Pipeline that have an associated verification step<=100%\n\nThere should be a mechanism to identify deviations, and if deviations from the standard are approved, then the system should account for (and manage) the exception as approved.\n\nThis metric should at least be aligned with an organization's development or release cycle to provide timely input for correction in the next deployment or release. For example, if an organization uses an Agile development methodology with two-week sprints, then the metric should be measured at least every two weeks to provide data for review at sprint retros.",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "AIS-07",
            "metricID": "AIS-07-M3",
            "metricDescription": "This metric measures the coverage for application vulnerability remediation across the production code",
            "metricExpression": "Percentage: 100 * A/B \nWhere:\nA = Number of deployed production applications with acceptable level of risk from application security vulnerabilities \nB = Total Number of deployed production applications",
            "metricRule": "Production Application = Applications tracked within the software inventory established in CCMv4 DCS-06\n\nAcceptable level of risk from application security vulnerabilities: Vulnerabilities categorized as medium or low risk as well as critical or high vulnerabilities marked or identified as 'Accepted' (i.e. remediation not required).  Examples of accepted vulnerabilities can be false positives or vulnerabilities with compensating controls that make the residual risk of exploitation acceptable.",
            "metricImplementationGuidelines": "There must be a Software Inventory of Deployed Production Code (see DCS-06 for more info). Production code must be quantified based on the organization's definition of deployed code running in production (e.g. microservices, builds, releases, packages, libraries, serverless functions, etc.). This should be the same number used to measure AIS-06.\n\nThe definition of \"deployed production application\" used for the software inventory should be aligned with application security scanning, testing, and/or reporting methods where possible to simplify measurement.\n\n\"Acceptable Level of Risk\" should be defined by the organizations vulnerability management guidelines (eg. only \"Critical\" and \"High\" vulnerabilities, or \"Medium Vulnerabilities and Higher,\" etc.  Classification of vulnerabilities as 'High' or 'Critical' risk, etc, should be defined in the Vulnerability Management tool based on industry-accepted scoring system such as the Common Vulnerability Scoring System (CVSS) (https://nvd.nist.gov/vuln-metrics/cvss). For instance, vulnerabilities with a CVSS score of 9 or higher are 'Critical', and vulnerabilities with CVSS scores between 7 and 9 could be defined as 'High' risk.)",
            "metricSLODescription": "80% \n\nRationale: The 2020 APPLICATION SECURITY OBSERVABILITY REPORT from Contrast Labs found 26% of applications had at least 1 serious vulnerability with 79% of those vulnerabilities remediated within 30 days. That leaves 20% of applications with serious vulnerabilities after 30 days, so the SLO to have 80% of production code with acceptable level of risk from application security vulnerabilities should be achievable for the average organization."
        },
        {
            "controlID": "AIS-07",
            "metricID": "AIS-07-M6",
            "metricDescription": "This Metric measures the percentage of critical vulnerabilities that are not fixed or marked as accepted within the time specified by policy",
            "metricExpression": "Percentage: A/B\nA= Number of unaccepted critical or high vulnerabilities with an age greater than the policy defined maximum age\nB= Total number of critical or high vulnerabilities within this period\n\ne.g. we ended up with: \nPercentage: 100 * 1-(A/B)\nA= Number of deployed production appliances with unaccepted critical or high vulnerabilities with an age greater than the policy defined maximum age \nB= Total Number of deployed production applications",
            "metricRule": "Production Application = Applications tracked within the software inventory established in CCMv4 DCS-06\n\nAcceptable level of risk from application security vulnerabilities: Vulnerabilities categorized as medium or low risk as well as critical or high vulnerabilities marked or identified as 'Accepted' (i.e. remediation not required).  Examples of accepted vulnerabilities can be false positives or vulnerabilities with compensating controls that make the residual risk of exploitation acceptable.",
            "metricImplementationGuidelines": "1. Classification of vulnerabilities as 'High' or 'Critical' risk should be defined in the Vulnerability Management tool based on industry-accepted scoring system such as the Common Vulnerability Scoring System (CVSS) (https://nvd.nist.gov/vuln-metrics/cvss). For instance, vulnerabilities with a CVSS score of 9 or higher are 'Critical', and vulnerabilities with CVSS scores between 7 and 9 could be defined as 'High' risk.\n\n2. Date and time of vulnerability discovery could be obtained from the Vulnerability Management tool as it scans and detects vulnerabilities.\n\n3. Date and time of vulnerability remediation or acceptance could be obtained in the following ways:\na) From the Vulnerability Management tool as it scans and finds that a previously detected vulnerability is no longer present/detected.\nb) From the patch deployment tool (e.g. SCCM) as it successfully deploys and installs a patch that fixes an identified vulnerability.\nc) From the application / code release tool as it moves into production the new version of the application that no longer contains the code vulnerability.\n\nFrequency of evaluation should be aligned with the frequency of vulnerability scans. (Scans should happen at LEAST monthly, but more frequently is recommended)\nVulnerability scans can be done at a predefined frequency or whenever new code is built, or deployed.",
            "metricSLODescription": ""
        },
        {
            "controlID": "BCR-06",
            "metricID": "BCR-06-M1",
            "metricDescription": "This metric reports the percentage of critical systems that passed BCR tests.",
            "metricExpression": "Percentage: 100*A/B \nWhere:\nA = Number of critical systems that passed BCR tests during the sampling period.\nB = Total number of critical systems operating during the sampling period.\n\nBCR refers to CCMv4 domain \"Business Continuity Management and Operational Resilience\".",
            "metricRule": "Criteria for system criticality must be defined and there must be a list of critical systems identified.\n\nRecovery Point Objective(s) and Recovery Time Objective(s) must be defined for critical systems. This metric does not attempt to measure the appropriateness of the recovery point or time objectives (RPOs or RTOs). This metric is dependent on control BCR-02 providing reasonable assurance of sufficient RPOs and RTOs for critical systems.\n\nBCR testing intervals must be defined.",
            "metricImplementationGuidelines": "Critical systems should be identified in accordance with the CCMv4 implementation guidelines for BCR-02.\n\nFor this metric, \"passed\" means achieving the RPO(s) within the RTO(s) defined for each critical system in the scope of the assessment/audit, according to the CCMv4 implementation guidelines for BCR-02.\n\nThe sampling period for this metric should align with the testing intervals defined by the business continuity plan in accordance with the CCMv4 implementation guidelines for BCR-04.\n\nBCR tests should include Chaos Testing, where possible. \"Chaos engineering is the discipline of experimenting on a software system in production in order to build confidence in the system's capability to withstand turbulent and unexpected conditions.\" -- wikipedia",
            "metricSLODescription": "80%\n\nBCR/Chaos testing is intended to be a learning activity, and it should test both the core of the system and the edges of the system,.  A perfect score indicates that edge cases and previously-undefined scenarios are not being tested.  Too low of a score indicates that an organization hasn't learned from their tests.  New tests should be continually added, old tests may be retired.  This metric should show regular variability."
        },
        {
            "controlID": "CCC-03",
            "metricID": "CCC-03-M1",
            "metricDescription": "Percentage of all Assets that have change management technology integrated",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of assets that have change management technology integrated\nB = Total number of assets.",
            "metricRule": "Change management technology covers release management tools that enable automated deployment and rollback of software builds in production",
            "metricImplementationGuidelines": "This metric requires the implementation of CCMv4 DCS-06 \"Asset Cataloging and Tracking\" and the capability to determine which assets or asset groups are deployed using change management technology that can rollback changes and/or stop deployment of risky changes based on automated test results. \n\nGiven the dynamic nature of cloud environments, the metric can provide more value if the variations in the release management system's coverage over the population of assets is reported over time. The percentage of assets that fall within an accepted number of deviations provides stakeholders assurance of whether change control is getting better, worse, or being maintained. Larger populations of more than 1,000 assets can use 6 standard deviations as an acceptable level of change over time (i.e. Six Sigma). Smaller populations of assets will need to use less standard deviations as an acceptable level of change, perhaps even just 1 deviation. For more information on the use of standard deviation in security metrics, see this excerpt of Andrew Jaquith's Security Metrics book.",
            "metricSLODescription": "80%\n\nThis provides flexibility for organizations to move quickly. The signal is if this measure is going down or going up. The exact level is a measure of the organization's risk tolerance."
        },
        {
            "controlID": "CCC-07",
            "metricID": "CCC-07-M1",
            "metricDescription": "This metric measures the percent of positive test results from all configuration tests performed",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of configuration items that were tested and passed successfully \nB = Total number of configuration items that were tested",
            "metricRule": "This metric captures the number of tests passed out of the total number of tests defined. Each test is assumed to verify a 'configuration item' which is arbitrarily defined as any component for which a test can be defined.",
            "metricImplementationGuidelines": "This metric assumes that CCC-03 has been successfully implemented and thus assumes that enough configuration items, at least in terms of number of CCMv4 DCS-06 assets, have change management technology to make this metric meaningful.  \n\nThis metric does not take into account a measure of risk for the configuration tests that have failed. The resulting flat percentage may not tell the full story of risk incurred from a control failure.  Future work may incorporate risk measures such as \"high and critical\" configuration tests. \n\nThe frequency of reporting this metric should tie in to the frequency of deployments/expected changes, minimally once a week. This metric should be measured on an automated, continuous basis.\n\nSince the scope is under the control of the organization, the metric results should be relatively high. The signal from this metric is that the existing system for change management is working or failing. A low percentage may not indicate a significant cybersecurity risk, but it may be a leading indicator of future security risk if the practice doesn't improve.\n\nThis is different than IVS-04 which measures the number of hardening tests against all assets.",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "CEK-03",
            "metricID": "CEK-03-M2",
            "metricDescription": "This metric measures if the cryptographic module continues to be up to 'approved standards'",
            "metricExpression": "Percentage: 100 * A/B \nWhere:\nA = Number of assets responsible for data at-rest or in-transit where the cryptographic library has passed Automated Cryptographic Validation Protocol tests or equivalent tests \nB = Total number of assets responsible for data at-rest or in-transit.",
            "metricRule": "",
            "metricImplementationGuidelines": "This leverages asset management and off the shell automated functionalities while allowing for flexibility against policy (which has previously passed CCMv4 CEK-01 audit).\n\nSee AVCP: https://csrc.nist.gov/Projects/Automated-Cryptographic-Validation-Testing",
            "metricSLODescription": "85%\n\nSQO is the expression output (a % remediated w/in policy specified time constraints). As this is an important aspect of functionality targets should be around 85%"
        },
        {
            "controlID": "CEK-04",
            "metricID": "CEK-04-M1",
            "metricDescription": "This metric measures the percentage of assets with cryptographic functions that meet an organization's defined cryptographic requirements",
            "metricExpression": "Percentage: 100 * A/B \nWhere:\nA = Number of assets with a cryptographic function that meets cryptographic requirements \nB = Total number of assets with a cryptographic function.",
            "metricRule": "The specification should be reported for all the adopted cryptographic suites.",
            "metricImplementationGuidelines": "For a Minimum Viable Product, the scope of evaluation may be limited to public-facing services, in which case a scan of all externally facing assets should be made and the scanned values compared against the requirements of the policy.\n\nThe SLO used for this metric may need to be increased or decreased based on the scope of assets covered by the metric.\n\nThis metric depends on the data classification tool in CCMv4 DSP-03, and requires that an organization determine the appropriate level of encryption for each classification, then requires comparison of the expected encryption applied versus the actual encryption applied, and reports on the difference.\n\nCCMv4 IPY-03 covers a subset of this measurement.",
            "metricSLODescription": "0.9"
        },
        {
            "controlID": "DCS-06",
            "metricID": "DCS-06-M1",
            "metricDescription": "This metric measures the ratio of managed assets (i.e. catalogued and tracked) to detected assets. The goal is to provide a signal if the asset cataloging and tracking system stops working.",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of distinct assets seen in security audit logs during the sampling period that are in an asset catalogue.\nB = Number of distinct assets seen in security audit logs during the sampling period.",
            "metricRule": "The assumption is that the design of the DCS-06 control process(es) was found to be effective by internal or external audits.",
            "metricImplementationGuidelines": "This relies on the security audit logs as defined in CCMv4 LOG-05 and the asset catalog defined in CCMv4 DCS-06.\n\nThis assumes LOG-05 is inclusive of logs of a number of events such as network traffic, network scanning, physical asset inventory.\nIt assumes that the logs include network traffic logging, and logs from other assets, and are sufficient to detect unexpected assets. We assume \"\"everything that is worthy is logged\"\" This is a dependency on the auditor to ensure the logging is \"\"complete enough\"\".\n\nThis is consistent with the metric for UEM-04; implementers may benefit from the similarities.\n\nThe following is likely dependent on the STA-01 - STA-06 and the SSRM. As those mature perhaps: \nAny 3rd party CSP's used by the organization where shared responsibility of controls resides in the organization should be included as logical assets for this catalog. For example if a CSP provides a microservice inherent in the operations of an offering that microservice is a logical asset. This ensures that metrics where DCS numbers are used in the denominator include those micro-services. This is intended to ensure the 'coverage' is accurate and inclusive of 3rd party CSPs where the organization is responsible for the controls.",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "DSP-04",
            "metricID": "DSP-04-M2",
            "metricDescription": "This metric measures the ratio of data assets that have been classified according to data classification policies specific to each organization. An organization may have a predefined list of data types (e.g. health care record, payment card record, identification number, etc.) and / or data sensitivity levels (e.g. Confidential, Internal Use Only, Public).",
            "metricExpression": "Percentage:  100 * A/B\nWhere:\nA = Total number of data records classified by type and/or sensitivity.\nB = Total number of data records stored.",
            "metricRule": "- Total number of records classified by type and/or sensitivity - this is a count of all data assets that have a defined classification by type or sensitivity level ('Undefined' classifications are not counted for this variable).\n- Total number of records stored - this is a count of all data assets that have been collected and are stored in the system such as DSP-03.\n- This metric measures data in terms of distinct data records, not distinct data types.",
            "metricImplementationGuidelines": "All data records must have corresponding metadata related to its data type and / or sensitivity. A defined list of data types and sensitivity levels must be defined. Records that do not meet any of the data classification types or sensitivity levels will have an 'undefined' classification and are not considered as 'classified' for this metric.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "DSP-04",
            "metricID": "DSP-04-M3",
            "metricDescription": "This metric measures the ratio of assets in the asset catalog that have been classified according to data classification policies specific to each organization. An organization may have a predefined list of data types (e.g. health care record, payment card record, identification number, etc.) and / or data sensitivity levels (e.g. Confidential, Internal Use Only, Public).",
            "metricExpression": "Percentage:  100 * A/B\nWhere:\nA = Total number of assets in the asset catalog that are classified by type and/or sensitivity of the data on that asset.\nB = Total number of assets in the organization's asset catalog.",
            "metricRule": "- Total number of assets classified by type and/or sensitivity of the data contained on the asset - this is a count of all assets that have a defined classification by type or sensitivity level ('Undefined' classifications are not counted for this variable).\n- Total number of assets - this is a count of all assets that have been collected and are stored in the system such as DSC-06.",
            "metricImplementationGuidelines": "All asset records must have corresponding metadata related to the type and / or sensitivity of data stored on the asset. A defined list of data types and sensitivity levels must be defined. Assets that do not contain data of the data classification types or sensitivity levels will have an 'undefined' data classification and are not considered as 'classified' for this metric.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "DSP-05",
            "metricID": "DSP-05-M1",
            "metricDescription": "This metric measures the percentage of records from the data inventory required by control DSP-03 (CCMv4) that are included in data flow documentation. Cloud service providers and their stakeholders can use this metric to determine whether the volume of data covered by the data flow documentation is sufficient or needs to be updated to satisfy defined business requirements.",
            "metricExpression": "Percentage: 100 * A/B\n\nWhere:\n\nA = Number of data records or data stores from the CCMv4 DSP-03 inventory correctly included in the data flow documentation\n\nB = Total number of data records or data stores in the CCMv4 DSP-03 inventory",
            "metricRule": "This metric can be measured by counting the number of records in a data store or by simply counting the data stores themselves.\n\n\"Correctly included\" means that the data record or data store is represented in the data flow documentation in accordance with the organization's defined requirements for representing inventories in the documentation. Generally, this means it exists in the documentation and is properly labeled with appropriate CCMv4 DSP-04 classifications if appropriate. \n\nNote: \nThe CCMv4 DSP-03 control objective is to \"Create and maintain a data inventory, at least for any sensitive data and personal data.\"\nThe CCMv4 DSP-04 control objective is to \"Classify data according to its type and sensitivity level.\"",
            "metricImplementationGuidelines": "This metric supports an incomplete DSP-03 inventory so long as it is a statistically significant random sampling of \"at least for any sensitive data and personal data\" (e.g. meets the DSP--03 control language objective). \n\nThis metric makes the assumption that the data flow diagram(s) is available in a machine-readable format but does not measure automated creation of the data inventory or the data flow documentation. The generation of the data flow document MAY be manual although the result MUST be digitized in order to perform automated comparisons against discovered data repositories. \n\nThis metric assumes the data flow documentation is in the form of a graph with nodes and edges where data stores are nodes in that graph. In order to count the number of records, there needs to be metadata with the number of records for each datastore. It measures the percentage of data stores (and their records) that are correctly captured as nodes in the graph.\n\nFor reference a \"data flow inventory\" similar to DSP-03 is required by CSA GDPR Code of Conduct Control #5 Data Transfer. \n\nThis should be evaluated every 2 weeks, or in accordance with the organization's development release cycles.",
            "metricSLODescription": "0.8"
        },
        {
            "controlID": "DSP-05",
            "metricID": "DSP-05-M2",
            "metricDescription": "This metric measures the percentage of data streams from the data inventory required by control DSP-03 (CCMv4) that are included in the data flow documentation. Cloud service providers and their stakeholders can use a metric like this to determine whether the different uses of data covered by the data flow documentation is sufficient or needs to be updated to satisfy defined business requirements.",
            "metricExpression": "Percentage: 100 * A/B\n\nWhere:\n\nA = Number of data streams from the CCMv4 DSP-03 inventory correctly included in the data flow documentation\n\nB = Total number of data streams in the CCMv4 DSP-03 inventory",
            "metricRule": "\"Data streams\" are the connections from data sources to data consumers illustrated in data flow diagrams. These connections should be included in the data inventory required by control DSP-03 (CCMv4). This may be a complete inventory of all data streams or a reasonable sample of data streams.",
            "metricImplementationGuidelines": "This metric supports an incomplete inventory of data streams so long as it is a reasonable sampling of streams for \"at least for any sensitive data and personal data\" (e.g. is intended to measure flows of data of the types that meet the DSP-03 control language objective regarding data inventories). Sampled data streams should be captured from live data streams of user and system activities.\n\nThis metric assumes the data flow documentation is available in a machine-readable format. The generation of the data flow document MAY be manual although the result MUST be digitized in order to perform automated comparisons against discovered data flows.\n\nFor reference a \"data flow inventory\" similar to DSP-03 is required by CSA GDPR Code of Conduct Control #5 Data Transfer. \n\nThis should be evaluated every 2 weeks, or in accordance with the organization's development release cycles",
            "metricSLODescription": "0.8"
        },
        {
            "controlID": "GRC-04",
            "metricID": "GRC-04-M1",
            "metricDescription": "This metric measures the effectiveness of the governance program's exception handling process.",
            "metricExpression": "Percentage: 100 * A/B \nWhere: \nA = Number of active policy exceptions where the time to resolution is within the documented timeline for resolution, during the sampling period.\nB = Total number of active policy exceptions, during the sampling period.",
            "metricRule": "An Exception Policy must be defined and must cover the entire lifecycle of an exception.\n\nActive policy exceptions that happen during the sampling period but which are not resolved yet are counted in B, not A.",
            "metricImplementationGuidelines": "This metric requires organizations to maintain records of policy exceptions that include the approval date and resolution date for calculation of mean time to resolution. The records could be as simple as entries in a spreadsheet or as complex as records for exception tracking in a GRC or vulnerability management system.\n\nThis metric also requires organizations to define the threshold(s) for acceptable resolution time(s). The definition could be as simple as a statement in a policy document that applies to all exceptions, or individually-defined target dates for resolution of each exception based on risk. In the case of the latter, the requirements for setting the target resolution date(s) should be established in a policy and the target date(s) will need to be tracked in the policy exception records.\n\nFor example if there is a ticketing system for remediation this tracks if the close date for the ticket was met.\n\nIf an org has very few exceptions then slipping on even one will dramatically affect their percentage. This is inherent in statistics and is not seen as a problem for now.",
            "metricSLODescription": "0.9"
        },
        {
            "controlID": "IAM-07",
            "metricID": "IAM-07-M1",
            "metricDescription": "This metric measures the percentage of users leaving the organization that were de-provisioned from the identity management system in compliance with the identity and access management policies.",
            "metricExpression": "Percentage of leavers de-provisioned in compliance with IAM policies: 100*A/B\n\nWhere\n\nA = Number of terminated users deprovisioned within policy guidelines during the sampling period\n\nB = Total Number of terminated users during the sampling period",
            "metricRule": "The time lapse between a user's termination and account deactivation must be measured in seconds.\n\nThe time lapse between user's termination and account deactivation = time stamp of account deactivation event - time stamp of employee termination or role change event recorded in the HR system",
            "metricImplementationGuidelines": "Steps to compute this may look like: \n1. Account deactivation timestamps can be obtained from the identity management system\n\n2. Employee termination or change event timestamps can be obtained from the Human Capital Information System (e.g. Workday).\n\n3. e.g. Count of [(User De-Provisioning date - User Termination Date) > Policy Duration)]/Total Number of Terminated Users for the period)\n\nThis metric only evaluates termination/deprovisioning events as an indicator of efficacy.  It does not measure job role change, which can be captured in IAM-08.\n\nThe recommended sampling period for this metric is monthly, but cloud service providers should ensure the sampling period aligns and frequency of evaluation align with their rate of change and risk tolerance.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "IAM-08",
            "metricID": "IAM-08-M2",
            "metricDescription": "This metric measures the time elapsed since the last recertification for all types of privileges (including user roles, group memberships, read/write/execute permissions to files/databases/scripts/jobs, etc). The metric returns the longest time identified. For example, if the longest time elapsed for a recertification of a privilege is 95 days. The metric will return this number. The value returned should not be greater than the frequency of privilege recertification or review defined in the organization's policies.",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of Accounts Reviewed with Correct Access in the Last 90 Days \nB = Total number of Accounts",
            "metricRule": "Date of last Recertification is the date and time that a privilege was reviewed and recertified in the most recent recertification. \n\nIf a Date of last recertification does not exist, this should be replaced with the Date a privilege was granted or an account was created.",
            "metricImplementationGuidelines": "The identity management system or system used to automate the account privilege recertification process (an example of this type of systems is Identity IQ by SailPoint), should maintain timestamps of account creations, privilege granting events (e.g. addition to user groups, granting of security roles, etc). These timestamps can be used to calculate this metric.\n\nOrphaned accounts (ie accounts that have not been terminated at the time of measurement in IAM-07) should be captured by the User Access Review described in IAM-08.\n\nThis captures any problem in the process such as: If i review a small number of account then i get a poor score. Or, if i review a large number of accounts and discover them to be in error, I get a bad score.\n\nThe measurement should be taken monthly to align with IAM-07, even if recertifications occur on a different periodicity.",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "IAM-09",
            "metricID": "IAM-09-M1",
            "metricDescription": "This metric measures the segregation of duties of non-production staff having access to production roles and vice-versa.",
            "metricExpression": "Percentage of users with segregation of privileged access roles: 100*(1-(A/B))\n\nWhere\n\nA = Number of users with admin access to more than one of the following capabilities: production data management, encryption and key management, or logging\n\nB = Number of users with access to production data management, encryption and key management, or logging capabilities",
            "metricRule": "Capabilities are privileged roles or functions. \n\nExamples of production data management capabilities are the AmazonRDSFullAccess policy in AWS, the Cloud SQL Admin & Cloud SQL Editor roles in the Google Cloud Platform (GCP), and db_owner role for a Microsoft Azure SQL Database.\n\nExamples of encryption and key management capabilities are the AWSKeyManagementServicePowerUser policy in AWS, Cloud KMS Admin with Cloud KMS CryptoKey Encrypter/Decrypter roles in GCP, or Microsoft Azure Key Vault Admin \n\nExamples of logging capabilities are the AWSCloudTrail_FullAccess policy in AWS, Monitoring Admin & Editor roles in GCP, or Monitoring Contributor in Microsoft Azure.",
            "metricImplementationGuidelines": "1. Identify privileged roles in an organization and map to the roles identified in this metric\n\n2. Run the metric across all users with privilege\n\nIn just-in-time (JIT) access capabilities, the audit should evaluate the ability for a user to be provisioned the privilege, even if the individual did not request the privilege during the measurement.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "IPY-03",
            "metricID": "IPY-03-M2",
            "metricDescription": "This metric measures the percentage of data flows that use an approved, standardized cryptographic security function for interoperable transmissions of data.",
            "metricExpression": "Percentage of data flows that use cryptographically secure and standardized network protocols: 100 * A/B\n\nWhere\n\nA = Count of data flows that use an approved, standardized cryptographic security function\n\nB = Count of all data flows",
            "metricRule": "This metric depends on a known inventory of data flows such as us required by DSP-05. This inventory may be built from IPY-02 and/or DSP-05 (see DSP-05-M2), or other options could exist (e.g. in some implementations a data flow might be counted as an asset type in a DCS-06 asset inventory). The count of all data flows is the count of items in the inventory used to satisfy DSP-05.\n\nApproved cryptographic security functions should be established by an organization policy or standard, as required by CEK-01. \n\nDetermining which data flows use an approved cryptographic security function can occur using analytics on the encrypted traffic flows or could occur by examining the associated configurations using the tooling from AIS-06 or from the CCC domain.",
            "metricImplementationGuidelines": "NIST 140-2 Annex A is a plausible set of interoperability-specific policy choices for standard cryptographic security functions. Other regions might drive different choices.\n\nThis metric should be a continuous measure over the previous hour. For example over the previous 1hr 86% of protocol flows were detected to be TLS 1.3 with selected cipher suites, or gRPC, or remote access VPN, or other types within the current policy set and listed in an interoperability specific policy to ensure interoperability.",
            "metricSLODescription": "0.9999"
        },
        {
            "controlID": "IVS-04",
            "metricID": "IVS-04-M1",
            "metricDescription": "This metric measures the percentage of assets in compliance with the provider's configuration security policy and hardening baselines derived from accepted industry sources (e.g. NIST, Vendor Recommendations, Center for Internet Security Benchmarks, etc).",
            "metricExpression": "Percentage: 100 * A/B \nWhere\nA = Number of production assets that are in compliance with hardening baseline\nB = Total number of production assets",
            "metricRule": "Examples of hardening baselines include Center for Internet Security Benchmarks, DISA STIGs, vendor recommended best practices, NIST security guidance, etc.",
            "metricImplementationGuidelines": "This metric of \"assets that are in compliance\" is inclusive of assets that have failed an initial test and where remediation is still within the SLA timeframe. If an asset is not fixed within the timeframe then it impacts the metric.\n\nHardening baselines derived from accepted industry sources (e.g., NIST, Vendor Recommendations, Center for Internet Security Benchmarks, etc) and in compliance with the provider's configuration security policy are expressed in test code which is run against the targeted asset on a regular basis.\n\nIf an asset fails these tests then an alert is generated and the team is expected to fix the problem within a policy defined SLA timeframe (likely inclusive of risk thresholds for various timeframes).",
            "metricSLODescription": "0.9999"
        },
        {
            "controlID": "LOG-03",
            "metricID": "LOG-03-M1",
            "metricDescription": "This metric measures the percentage of logs configured to generate security alerts for anomalous activity across control domains such as Application & Interface Security, Business Continuity Management, Change Control & Configuration Management, Identity & Access Management, Infrastructure & Virtualization Security, Threat & Vulnerability Management, and Endpoint Management.",
            "metricExpression": "Percentage of logs configured with security alerts: 100 * A/B\n\nWhere:\nA = Number of Log Sources with Security Alerts configured\nB = Total Number of Log Sources",
            "metricRule": "Log sources can be the system log(s) or input(s) to the logging pipeline(s)\n\nSecurity alerts include traditional alerts triggered when a log records events in a control domain above a specified threshold as well as alerts generated by anomaly detection using machine learning",
            "metricImplementationGuidelines": "This metric measures alerts based on items of interest occuring within a log.\n\nThis metric requires CSPs to have an inventory of log sources or inputs for their logging pipeline(s) and the ability to determine a unique count of those log sources or inputs to the logging pipeline with anomaly detection or security alerts configured for them.\n\nAn example implementation may look like this: \n\nLogs -> Log Analytics Engine -> Log Alerts Engine -> SIEM/ITSM",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "LOG-05",
            "metricID": "LOG-05-M1",
            "metricDescription": "This metric reports the effectiveness of the log monitoring and response process by measuring the percentage of discovered anomalies resolved within required timelines.",
            "metricExpression": "Percentage of anomalies resolved in compliance with policy: 100 * A/B if B is not 0, or 100 when B is 0\n\nWhere: \nA = Number of anomalies detected during the sampling period that were reviewed and resolved within a timeframe that is in compliance with Policy. \nB = Total number of anomalies detected during the sampling period.",
            "metricRule": "Anomalies that have been detected during the sampling period, but have not been reviewed and resolved during the sampling period are not counted in A. \n\nAn anomaly is any event happening outside of typical or expected patterns.",
            "metricImplementationGuidelines": "Activity \"outside of typical or expected patterns\" is something for the CSP to define. A common mechanism is to use Indicators of Compromise to detect anomalies. For example https://docs.oasis-open.org/cti/stix/v2.1/cs01/stix-v2.1-cs01.html#_muftrcpnf89v \n\nIf no anomalous events are detected during the sample period, the resulting metric (a divide by zero error) is not included in the metrics reported.",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "LOG-10",
            "metricID": "LOG-10-M1",
            "metricDescription": "This metric measures the percentage of cryptography, encryption and key management controls with defined metrics",
            "metricExpression": "Percentage of encryption controls with defined metrics: 100 * A/B\n\nWhere:\nA = Number of metrics reported in the CCMv4 CEK domain.\nB = Total number of controls in the CCMv4 CEK domain",
            "metricRule": "",
            "metricImplementationGuidelines": "This requires defining metrics beyond the minimal set currently defined in order to meet the recommended SLO\n\nMetrics for all CEK controls may not be easily automated, for example CEK-01, CEK-02, CEK-06, CEK-07, and CEK-08.\n\nThis is against the total number of controls in the CEK domain, rather than the # of controls asserted as met in the last audit. This simplifies the metric as the implementers do not need programmatic access to the previous audit results. \n\nGenerally the recommended frequency should be the maximum frequency recommended for CEK metrics",
            "metricSLODescription": "0.8"
        },
        {
            "controlID": "LOG-13",
            "metricID": "LOG-13-M2",
            "metricDescription": "This metric measures \"failures [e.g. uptime] of the monitoring system\". The other aspects of this control such as \"reporting of anomalies\" and \"immediate notification to the accountable party\" are to be measured using other metrics.",
            "metricExpression": "Percentage of uptime: 100 * A/B\n\nWhere:\nA = Number of minutes of uptime during the sampling period\nB = Duration of the sampling period in minutes",
            "metricRule": "Uptime = (total number of minutes in the sampling period - downtime in minutes during the sampling period) \n\nDowntime = any minute where health checks for any component of the monitoring system failed",
            "metricImplementationGuidelines": "\"Minutes\" provides sufficient granularity to measure uptime up to a target of 5 9's. It should be noted though that the recommended frequency of evaluation is daily rather than yearly and therefore five nines score during any particular day can not be extrapolated as a yearly uptime. This reflects the objective of measuring and reporting on potential failures of the monitoring system for \"immediate\" notification at least daily. \n\nTo determine if a system is \"up\" a health check is expected. This metric does not mandate a specific healthcheck. Many uptime monitoring solutions exist that can be used as implementation examples or as services. A recommended level of health check is one that tests the functionality of the monitoring system during the minute of the test. For example a simple TCP/IP ping measures \"uptime\" but is insufficient to measure the availability of the functionality of a monitoring system. Testing that log entries are being persistently recorded during that minute is a more accurate measure of uptime availability. \n\nThe LOG-03 monitoring and alerting objective can reasonably be met by deploying multiple monitoring and alerting systems that are responsible for different areas of a complex environment. If multiple independent monitoring systems are deployed and only one fails a health check during any given minute is the system as a whole \"up\" or \"down\" during that minute? For the purpose of this metric, if any monitoring and alerting system fails a health check during a minute then the system as a whole is considered to be \"down\" during that minute. This is simplistic and easy and accurately captures the increased complexity of running multiple monitoring systems.\n\nThis simplification does not however support considerations like \"this subset monitoring and alerting system only covers a small number of low risk elements of the infrastructure\". Future versions of this metric may include \"coverage\" or \"risk\" elements to the metric expression.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "SEF-05",
            "metricID": "SEF-05-M1",
            "metricDescription": "This metric measures the percentage of security events sourced from automated systems",
            "metricExpression": "Percentage of security events from automated systems: 100 * A/B\n\nWhere:\nA = Number of Security Events Sourced from Automated Systems during the sampling period\nB = Total Number of Security Events that were recorded during the sampling period.",
            "metricRule": "The log sources configured with security alerts for the LOG-03-M1 metric are examples of automated systems.",
            "metricImplementationGuidelines": "Automated systems include logging and monitoring systems as well as systems that generate alerts for review, including threat intelligence systems.\n \nSecurity events manually entered by individuals or organizations for triage are from \"non-automated systems\", e.g. vulnerability disclosure emails, security event tickets created by staff or customers, etc.",
            "metricSLODescription": "0.9"
        },
        {
            "controlID": "SEF-06",
            "metricID": "SEF-06-M1",
            "metricDescription": "This metric measures the percentage of security events triaged within policy timeframe targets.",
            "metricExpression": "Percentage of security events triaged in compliance with policy: 100 * A/B \n\nWhere\nA = Number of Security Events triaged within policy defined time limit, during the sampling period\n\nB = Total Number of Security Events Logged, during the sampling period",
            "metricRule": "Policy targets as established in CCMv4 SEF-01 are used here as a proxy for \"within a reasonable time\". This metrics is manipulatable by selecting an easy to achieve policy target but doing so should create friction during the initial audit.",
            "metricImplementationGuidelines": "Events occur and are classified as part of triage process. This can occur automatically and/or there can be manual triage steps. Once the event reaches its final categorization it is \"triaged\". As long as this completes within the organization's target time period it is \"within the SLO\".\n\nIt may be aggressive for a small organization that does not have a lot of events to report this metric frequently.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "SEF-06",
            "metricID": "SEF-06-M2",
            "metricDescription": "This metric indicates if security event triage process times are stable, improving, or worsening.",
            "metricExpression": "Slope represented as a percentage:\nA = SLOPE(triage times for security events, dates for security events) * 100",
            "metricRule": "The SLOPE is the representation of the linear regression of the triage times as graphed against the dates (or sequence numbers) for security events within the time period.",
            "metricImplementationGuidelines": "Events occur and are classified as part of triage process. This can occur automatically and/or there can be manual triage steps. Once the event reaches its final categorization it is \"triaged\". As long as this completes within the organizations target time period it is \"within the SLO\".\n\nThe slope of time to triage indicates if the event triage process has improved, stayed the same, or increased (worsened).\n\nThis metric does not capture if the triage time is within a specific policy target. It only captures that the organization has in fact defined, implemented, and has a process for evaluating their triage process. This meets the objective of the control.\n\nThis can be implemented in spreadsheets as the \"SLOPE\" function w/in formulas and charts (see Excel or Sheets). See for example https://support.google.com/docs/answer/3094048?hl=en",
            "metricSLODescription": "< 0\n\nA slope of 0 means the triage process is stable\nA slope of <0 means the triage process is improving\nA slope of >0 means the triage process is worsening"
        },
        {
            "controlID": "STA-07",
            "metricID": "STA-07-M3",
            "metricDescription": "The % of third-party software components seen in production assets that are sourced from an approved supplier in the software inventory",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Total number of third-party software components \"seen\" during the sampling period, which are from authorized providers.\nB = Total number of third-party software components \"seen\" during the sampling period\"",
            "metricRule": "",
            "metricImplementationGuidelines": "A software component is a discrete unit of software, such as a library or package, with uniquely identifiable attributes such as would be supplied in a Software Bill of Materials (SBOM).\n\nA simplistic approach is to track all software libraries and ensure they are in the inventory of approved libraries. One could 'track' by using the vendor supplied SBOM or by scanning the software components directly. The implementer should have sufficient context in the STA-07 inventory to determine if a listed component library is approved, not-approved, or unknown (e.g. new and must be evaluated). \n\nA more granular approach is to use context to determine if the software should be running on this particular asset. For example: Bastion (jumphost) software or libraries may be approved for use on a hardened bastion asset but may not be appropriate for a non-hardened asset. \n\nThe use of \"seen\" allows for sampling. There is nothing currently exposed in the metric to expose how statistically significant the sampling was. It's assumed that an initial audit confirmed significant sampling was used.",
            "metricSLODescription": "0.999"
        },
        {
            "controlID": "STA-07",
            "metricID": "STA-07-M5",
            "metricDescription": "The % of approved supply chain Upstream Cloud Services relationships that are not recorded in logged data connections",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = The total number of Unique Providers with Observed Connections\nB = Total Number of Unique Providers in the inventory\nBoth A and B are measured over the same sampling period.",
            "metricRule": "",
            "metricImplementationGuidelines": "This measurement requires a list of Cloud Service Provider Connections that are Approved and Expected and an ability to log all connections to expected endpoints of those providers.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "TVM-03",
            "metricID": "TVM-03-M1",
            "metricDescription": "This metric measures the percentage of high and critical vulnerabilities that are remediated within the organization's policy timeframes. This reflects the time between when a vulnerability is identified on an organization's assets and when remediation is complete.",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of high and critical vulnerabilities identified during the sampling period and remediated within policy timeframes\nB = Total Number of High and Critical Vulnerabilities identified during the sampling period",
            "metricRule": "High and critical Vulnerabilities are defined consistent with the implementation of TVM-08. \n\nIf a vulnerability is identified but not remediated yet when the measurement is made, the measurement date is used as the remediation date in order to evaluate if the vulnerability has been mitigated within the defined policy timeframe, as expected for the calculation of A.",
            "metricImplementationGuidelines": "To compute the denominator:\nThe \"Total Number of High and Critical Vulnerabilities\" are any such vulnerabilities that are still open from previous periods plus all such newly identified during the current sample period.  A minimal example framework for vulnerability prioritization is CVSS v.3.0 where \"High\" and \"Critical\" are defined as 7.0 and above.\n\nTo compute the numerator:\na) Fetch all Critical or High vulnerabilities newly identified during the current period\nb) Fetch all Critical or High vulnerabilities that are still Open (not Closed) from the previous period\n\nFor example assume the following data sets for three example weekly periods: \n\nExample period 1: \n04/25-05/01 - (a) Number of Critical + High Vulnerabilities Created during the period = 10\n                      (b) Number of Critical + High Vulnerabilities still Open from previous periods (as of the beginning of the current period-04/25) = 3\n                      (c) Number of Critical + High Vulnerabilities Closed during the period within Policy = 8\n                      (d) Total Number of Critical + High Vulnerabilities Closed = 8\n\nExample period 2: \n05/02-05/08 - (e) Number of Critical + High Vulnerabilities Created during the period = 15\n                      (f) Number of Critical + High Vulnerabilities still Open from previous period (as of the beginning of the current period-05/02) = 5 (e.g. a+b-d, but in an actual implementation this is possibly determined with a database query) \n                      (g) Number of Critical + High Vulnerabilities Closed during the period within Policy = 14\n                      (h) Total Number of Critical + High Vulnerabilities Closed = 20\n\nMetric for the period 05/02-05/08:\nNumerator = (g)/[(e) + (f)] = 14/(15+5) = 70%\n\nExample period 3:\n05/09-05/15 - (i) Number of Critical + High Vulnerabilities Created during the period = 5\n                      (j) Number of Critical + High Vulnerabilities still Open from previous periods (as of the beginning of the current period-05/02) = 0 (e+f-h)\n                      (k) Number of Critical + High Vulnerabilities Closed during the period within Policy = 4\n                      (l) Total Number of Critical + High Vulnerabilities Closed = 4\n\nMetric for the period 05/09-05/15:\nNumerator = (k)/[(i) + (j)] = 4/(5+0) = 80%",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "TVM-07",
            "metricID": "TVM-07-M1",
            "metricDescription": "This metric measures the percentage of managed assets scanned monthly",
            "metricExpression": "Percentage: 100 * A/B \nWhere:\nA = Number of assets from the organization's asset catalog that have been scanned during the sampling period\nB = Total number of assets in the organization's asset catalog",
            "metricRule": "The \"asset catalog\" refers to the cataloging requirements of CCMv4 DCS-06, which requires to \"Catalogue and track all relevant physical and logical assets located at all of the CSP's sites within a secured system.\"",
            "metricImplementationGuidelines": "This metric requires organization-managed assets to be maintained in the catalog required by control DCS-06 in CCMv4. The asset catalog must be integrated with the vulnerability management process to track when assets in the catalog are scanned.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "TVM-10",
            "metricID": "TVM-10-M1",
            "metricDescription": "This metric measures the percentage of publicly known vulnerabilities that are identified for an organization's assets within the organization's required timeframes. The purpose of this metric is to determine how long it takes an organization to start tracking vulnerabilities for triage. This measure is important because Palo Alto Networks reported that Internet assets are scanned once every 15 minutes or less after CVEs are published. This metric does not include the time to remediation (which is measured by TVM-03-M1).",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of High and Critical Vulnerabilities identified for remediation within policy timeframes\nB = Total Number of High and Critical Vulnerabilities identified or carried-over into the sampling period",
            "metricRule": "To compute the NUMERATOR: Determine the Total Number of Critical and High Vulnerabilities that have been identified for remediation per TVM-01 policies and procedures. In order to compute the metric, use the following logic:\nFor each Critical or High Vulnerability that were Identified during the period or carried forward from the previous period\na) Check the Vulnerability Publish Date. Is this date < the date on which the Asset was commissioned. If so, use the Asset Commission Date as the \"from date\"; if not, use the Vulnerability Publish Date as the \"from date\"\nb) Subtract the Vulnerability Identification Date from the \"from date\". The Identification Date is the date that your organization has acknowledged the vulnerability to be acted upon (e.g. this may be a the ticket create date on Jira for the given vulnerability)\nc) Evaluate if #b > the policy duration (in days). If so, add +1 to the count\n\nTo compute the DENOMINATOR:\nThe \"Total Number of High and Critical Vulnerabilities\" are:\na) Opened during the current period and/or\nb) Carried over from the previous period because the policy timeframe spans period. For example, let us say that we measure the control on a weekly basis: Sunday - Saturday, and the policy timeframe is 3 days. Any issue identified on or after Thu is not due, per policy, until the following period. These vulnerabilities are \"carried over\" into the following period.",
            "metricImplementationGuidelines": "1. Classification of vulnerabilities as 'High' or 'Critical' risk should be defined in the Vulnerability Management tool based on industry-accepted scoring system such as the Common Vulnerability Scoring System (CVSS) (https://nvd.nist.gov/vuln-metrics/cvss) or risk-based vulnerability management system (see https://tinyurl.com/what-is-rbvm). For instance, vulnerabilities with a CVSS score of 9 or higher are 'Critical', and vulnerabilities with CVSS scores between 7 and 9 could be defined as 'High' risk.\n\n2. Date and time of vulnerability discovery could be obtained from the Vulnerability Management tool as it scans and detects high and critical vulnerabilities for remediation.\n\n3. Date and time of vulnerability remediation could be obtained in the following ways:\na) From the Vulnerability Management tool as it scans and finds that a previously detected vulnerability is no longer present/detected.\nb) From the patch deployment tool (e.g. SCCM) as it successfully deploys and installs a patch that fixes an identified vulnerability.\nc) From the application / code release tool as it moves into production the new version of the application that no longer contains the code vulnerability.\n\nThis metric depends on a policy timeline target for the identification (completion of the triage process) for known vulnerabilities.",
            "metricSLODescription": "none"
        },
        {
            "controlID": "UEM-04",
            "metricID": "UEM-04-M1",
            "metricDescription": "This metric provides an indication of endpoints that are actively maintained in the Asset Inventory",
            "metricExpression": "Percentage: 100 * A / B\nWhere:\nA = Number of endpoints that meet the following two conditions simultaneously:\nA1: The endpoint is seen in security audit logs as a result of activities outside of typical or expected patterns. (cf. CCMv4 LOG-05)\nA2: The endpoint is in the company's inventory of all endpoints used to store and access company data (cf. CCMv4 UEM-04). \nB = The total number of endpoints in the company's inventory all endpoints used to store and access company data  (cf. CCMv4 UEM-04).\n\nThe datasets referred in A2 and in B are the same. \nCondition A1 is limited to security log events that happened during the sampling period.",
            "metricRule": "The data used in the expression is all data from the control period.\n\nThis metric assumes that the following two CCMv4 controls are in place (or an equivalent):\n- LOG-05: Monitor security audit logs to detect activity outside of typical or expected patterns. \n- UEM-04: Maintain an inventory of all endpoints used to store and access company data.",
            "metricImplementationGuidelines": "the data used in the expression is all data from the control period.\n\nThis assumes the LOG-05 logs (\"security audit logs to detect activity outside of typical or expected patterns\") are inclusive of endpoint identities. \n\nThe frequency of evaluation for UEM-04 has to match how long log data is maintained for. \n\nExamples of endpoints which store or access company data include end-user devices, point of sale systems, databases, IoT systems, and data integration systems.",
            "metricSLODescription": "0.95"
        },
        {
            "controlID": "UEM-05",
            "metricID": "UEM-05-M1",
            "metricDescription": "This metric describes the ability of an organization to control the configuration and behaviour of assets which directly create, read, write, or delete organizational data",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = Number of unique endpoints with suitable policy enforcement tools that have reported compliance state within the sampling period\nB = The total number of endpoints is in the company's inventory all endpoints used to store and access company data  (cf. CCMv4 UEM-04).",
            "metricRule": "This metric assumes that the following CCMv4 control are in place (or an equivalent):\n- UEM-04: Maintain an inventory of all endpoints used to store and access company data.\n\nThe capability to measure and enforce Desired Configuration and/or Policy must be a discrete, measureable entity, such as an agent, an implementation constraint (for example containers or read-only filesystems), or an external control (such as network authentication and access policies or software defined networking).\n\nIn order to provide this measurement the discrete capability must be an approved mechanism or mechanisms whose implementation is mandated by policy.\n\nIn order for this measurement to be meaningful, UEM-04 must have a measurement greater than 95%",
            "metricImplementationGuidelines": "If there are devices that are in an exception group they still count as a policy control being applied for the purposes of this metric. \n\nThis metric does not differentiate between partial reporting and full reporting of all of the policies from a given system, but only the capability of that system to report.\n\nSee UEM-04 for examples of systems which access or store organizational data.\n\nTechnical measures to enforce policies and controls for endpoints include API tools such as OSQuery, DCM tools, MDM tools, VPN Access Policy Controls, etc.",
            "metricSLODescription": "0.99"
        },
        {
            "controlID": "UEM-09",
            "metricID": "UEM-09-M1",
            "metricDescription": "This metric measures the percentage of instances which are an running anti-malware/virus service.",
            "metricExpression": "Percentage: 100 * A/B\nWhere:\nA = The total number of managed endpoints from employee devices initiating observed connections where a device check inclusive (of verifying malware protection) as been passed\nB = The total number of managed endpoints from employee devices initiating observed connections",
            "metricRule": "\"device check\" is some form of posture assessment during connection or path establishment. Some examples: \n* Trusted Platform Module posture assessment \n* VPN posture assessment\n* ZTNA / MF posture assessment\n* perform \"out of band\" checks by correlating connection log information with independent posture assessment monitoring",
            "metricImplementationGuidelines": "This depends on an Asset Database such as from CCMv4 DCS-06.  The targeted classifications of assets in scope must be identified in CCMv4 DCS-05 (e.g. \"employee devices\")",
            "metricSLODescription": "0.99"
        }
    ]
}